{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-08T00:11:59.377582Z",
     "start_time": "2024-09-08T00:11:58.945428Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T00:10:55.403305Z",
     "start_time": "2024-09-08T00:09:33.973284Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_223025/2456223005.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_res_train = df_res_train.groupby('job_title', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94208 39997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_223025/2456223005.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_res_train = df_res_train.groupby('job_title', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92680 39996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_223025/2456223005.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_res_train = df_res_train.groupby('job_title', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92903 39997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_223025/2456223005.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_res_train = df_res_train.groupby('job_title', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94054 39996\n",
      "78625 25745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_223025/2456223005.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_res_train = df_res_train.groupby('job_title', group_keys=False).apply(\n"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train = pd.DataFrame()\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "# Read all the csv files for that task\n",
    "for i in range(1, 6):\n",
    "    df_res = pd.read_csv(f'data/TRAIN_RES_{i}.csv')\n",
    "    # remove the job titles that appear less than 2 times to be able to split dataset with stratify\n",
    "    common_titles = df_res.job_title.value_counts()[df_res.job_title.value_counts() < 2].index\n",
    "    df_res.drop(df_res[df_res.job_title.isin(common_titles)].index, inplace = True)\n",
    "\n",
    "    # Cleanup: drop the 'achievements' column because we have 'achievements_modified'\n",
    "    # Also replace 'Нет' in 'achievements_modified' with None\n",
    "    df_res.drop('achievements', axis = 1, inplace = True)\n",
    "    df_res['achievements_modified'] = df_res['achievements_modified'].replace('Нет', None)\n",
    "\n",
    "    # train test split\n",
    "    df_res_train, df_res_test = train_test_split(df_res, test_size = 0.2, random_state = 42, stratify = df_res['job_title'], shuffle = True)\n",
    "\n",
    "    # sample 20% each part of dataset for the test set\n",
    "    df_res_test = df_res_test.sample(frac = 0.1, random_state = 42)\n",
    "\n",
    "    # remove nan values\n",
    "    df_res_train.dropna(subset = ['achievements_modified', 'demands'], how = 'all', inplace = True)\n",
    "    \n",
    "    # If some job appears more than 200 times, sample 200 times. Else if \n",
    "    # some job appears less than 200 times, take all of them\n",
    "    df_res_train = df_res_train.groupby('job_title', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), 200), random_state=42)\n",
    "    )\n",
    "\n",
    "    print(df_res_train.shape[0], df_res_test.shape[0])\n",
    "    \n",
    "    # Concat to the global dataset\n",
    "    df_train = pd.concat([df_train, df_res_train])\n",
    "    df_test = pd.concat([df_test, df_res_test])\n"
   ],
   "id": "2467b9fbfbaf989f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T00:10:55.518420Z",
     "start_time": "2024-09-08T00:10:55.512650Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((452470, 5), (185731, 5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3,
   "source": "df_train.shape, df_test.shape",
   "id": "7ba310ee3a01e40a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T23:34:40.838298Z",
     "start_time": "2024-09-07T23:34:40.742443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# remove the job titles that appear less than 1000 times in the train set\n",
    "common_titles = df_train.job_title.value_counts()[df_train.job_title.value_counts() < 1000].index\n",
    "\n",
    "df_train = df_train[~df_train.job_title.isin(common_titles)]"
   ],
   "id": "dcff0efa7249395a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T23:34:40.862778Z",
     "start_time": "2024-09-07T23:34:40.848699Z"
    }
   },
   "cell_type": "code",
   "source": "df_train.job_title.value_counts()",
   "id": "c87a98f61831cd71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job_title\n",
       "эколог           1000\n",
       "экономист        1000\n",
       "экскурсовод      1000\n",
       "экспедитор       1000\n",
       "эксперт          1000\n",
       "                 ... \n",
       "агроном          1000\n",
       "адвокат          1000\n",
       "администратор    1000\n",
       "акушерка         1000\n",
       "аналитик         1000\n",
       "Name: count, Length: 248, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T23:34:40.949983Z",
     "start_time": "2024-09-07T23:34:40.921305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = df_train.drop('job_title', axis = 1)\n",
    "X_test = df_test.drop('job_title', axis = 1)\n",
    "y_train = df_train['job_title']\n",
    "y_test = df_test['job_title']"
   ],
   "id": "1d936a25ddef5c49",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T23:34:41.464775Z",
     "start_time": "2024-09-07T23:34:40.968434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Impute nans with empty string for simplicity\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(fill_value='', strategy='constant')\n",
    "\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns = X_train.columns)\n",
    "X_test = pd.DataFrame(imputer.fit_transform(X_test), columns = X_test.columns)"
   ],
   "id": "f81c37ebc364c667",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T23:34:41.928143Z",
     "start_time": "2024-09-07T23:34:41.482343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Concatenate the text features together\n",
    "X_train_concat = X_train.demands + ' ' + X_train.company_name + ' ' + X_train.achievements_modified\n",
    "\n",
    "X_test_concat = X_test.demands + ' ' + X_test.company_name + ' ' +  X_test.achievements_modified"
   ],
   "id": "66b3c7a2a8015614",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T23:34:44.079800Z",
     "start_time": "2024-09-07T23:34:44.076395Z"
    }
   },
   "cell_type": "code",
   "source": "X_train_concat.head()",
   "id": "253d33bfa5465ef3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Ремонт и техническое обслуживание автортранспо...\n",
       "1    Капитальный ремонт техники, разборка-сборка ав...\n",
       "2    Ремонт, обслуживание, ТО легковых автомобилей ...\n",
       "3                       Ремонт автомобилей АО РСК МиГ \n",
       "4    Официальный дилер. Ремонт и обслуживание автом...\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T23:34:44.548886Z",
     "start_time": "2024-09-07T23:34:44.141745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# preprocess the labels with one-hot encoding (will be used in the model training)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "y_train_enc = encoder.fit_transform(y_train.values.reshape(-1, 1)).toarray()\n",
    "y_test_enc = encoder.transform(y_test.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "# save the encoder\n",
    "with open('models/classification/one_hot_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder, f)"
   ],
   "id": "d18171cfcb39b216",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T23:34:44.575864Z",
     "start_time": "2024-09-07T23:34:44.573111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_cols = list(encoder.categories_[0])\n",
    "len(target_cols)"
   ],
   "id": "6b5cd91aacc80c26",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T00:07:30.541039Z",
     "start_time": "2024-09-08T00:07:30.483694Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 25,
   "source": [
    "# preprocess the labels with ordinal encoding (will be used in the model evaluation)\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "y_train_label = oe.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_label = oe.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# save the encoder\n",
    "with open('models/classification/ordinal_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(oe, f)"
   ],
   "id": "8b5a7932ee137fd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "b23b0e81e460b1cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T23:34:46.992313Z",
     "start_time": "2024-09-07T23:34:44.619820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/rubert-tiny2\")"
   ],
   "id": "9e1d782f8ada335f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T23:34:47.035651Z",
     "start_time": "2024-09-07T23:34:47.033544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Take a rubert-tiny2 and add a linear layer after. No freezing = train all the weights\n",
    "class BERTClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained(\"models/rubert-tiny2\")\n",
    "        self.fc = nn.Linear(312, len(target_cols))\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, features = self.roberta(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output = self.fc(features)\n",
    "        return output\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ],
   "id": "87ed6c9cc16abef3",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T23:34:48.270763Z",
     "start_time": "2024-09-07T23:34:48.269061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VALID_BATCH_SIZE = 64\n",
    "EPOCHS = 3"
   ],
   "id": "aa80defc6372c20b",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T23:34:48.294706Z",
     "start_time": "2024-09-07T23:34:48.291898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, max_len):\n",
    "        self.len = len(X)\n",
    "        self.X = X.reset_index(drop=True)\n",
    "        self.y = y\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.X.iloc[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.y[index], dtype=torch.float)\n",
    "        }"
   ],
   "id": "95ed3243b8d1f5a6",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T23:34:48.347903Z",
     "start_time": "2024-09-07T23:34:48.338871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = BERTDataset(X_train_concat, y_train_enc, tokenizer, MAX_LEN)\n",
    "valid_dataset = BERTDataset(X_test_concat, y_test_enc, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=4, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=4, shuffle=False, pin_memory=True)"
   ],
   "id": "1bdab95a1d68ed2f",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T23:34:48.399089Z",
     "start_time": "2024-09-07T23:34:48.396354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ],
   "id": "8abaca0694c045d",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T23:34:48.444733Z",
     "start_time": "2024-09-07T23:34:48.442257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _, data in enumerate(train_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%100 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}, processed: {_/len(train_loader)*100}%')\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ],
   "id": "5c8eeed94aaef1df",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T00:05:13.776839Z",
     "start_time": "2024-09-07T23:34:48.489917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ],
   "id": "bba03fbebc0abeec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  5.512273788452148, processed: 0.0%\n",
      "Epoch: 0, Loss:  4.937572479248047, processed: 2.5806451612903225%\n",
      "Epoch: 0, Loss:  4.456879615783691, processed: 5.161290322580645%\n",
      "Epoch: 0, Loss:  3.9377903938293457, processed: 7.741935483870968%\n",
      "Epoch: 0, Loss:  3.5537214279174805, processed: 10.32258064516129%\n",
      "Epoch: 0, Loss:  3.3198866844177246, processed: 12.903225806451612%\n",
      "Epoch: 0, Loss:  2.8302321434020996, processed: 15.483870967741936%\n",
      "Epoch: 0, Loss:  2.8316054344177246, processed: 18.064516129032256%\n",
      "Epoch: 0, Loss:  2.4199843406677246, processed: 20.64516129032258%\n",
      "Epoch: 0, Loss:  2.3450231552124023, processed: 23.225806451612904%\n",
      "Epoch: 0, Loss:  2.5673389434814453, processed: 25.806451612903224%\n",
      "Epoch: 0, Loss:  2.6115872859954834, processed: 28.387096774193548%\n",
      "Epoch: 0, Loss:  2.113712787628174, processed: 30.967741935483872%\n",
      "Epoch: 0, Loss:  1.9976942539215088, processed: 33.5483870967742%\n",
      "Epoch: 0, Loss:  2.1383304595947266, processed: 36.12903225806451%\n",
      "Epoch: 0, Loss:  2.154313802719116, processed: 38.70967741935484%\n",
      "Epoch: 0, Loss:  1.9014923572540283, processed: 41.29032258064516%\n",
      "Epoch: 0, Loss:  2.3225250244140625, processed: 43.87096774193549%\n",
      "Epoch: 0, Loss:  2.050909996032715, processed: 46.45161290322581%\n",
      "Epoch: 0, Loss:  2.001979351043701, processed: 49.03225806451613%\n",
      "Epoch: 0, Loss:  1.522090196609497, processed: 51.61290322580645%\n",
      "Epoch: 0, Loss:  1.49068021774292, processed: 54.19354838709678%\n",
      "Epoch: 0, Loss:  2.1191039085388184, processed: 56.774193548387096%\n",
      "Epoch: 0, Loss:  1.893376350402832, processed: 59.354838709677416%\n",
      "Epoch: 0, Loss:  1.7175666093826294, processed: 61.935483870967744%\n",
      "Epoch: 0, Loss:  1.5302269458770752, processed: 64.51612903225806%\n",
      "Epoch: 0, Loss:  1.7784905433654785, processed: 67.0967741935484%\n",
      "Epoch: 0, Loss:  1.6099659204483032, processed: 69.6774193548387%\n",
      "Epoch: 0, Loss:  2.1568238735198975, processed: 72.25806451612902%\n",
      "Epoch: 0, Loss:  1.8862965106964111, processed: 74.83870967741936%\n",
      "Epoch: 0, Loss:  1.9039297103881836, processed: 77.41935483870968%\n",
      "Epoch: 0, Loss:  1.7636816501617432, processed: 80.0%\n",
      "Epoch: 0, Loss:  1.9989004135131836, processed: 82.58064516129032%\n",
      "Epoch: 0, Loss:  1.6339633464813232, processed: 85.16129032258064%\n",
      "Epoch: 0, Loss:  1.9215127229690552, processed: 87.74193548387098%\n",
      "Epoch: 0, Loss:  1.2918298244476318, processed: 90.32258064516128%\n",
      "Epoch: 0, Loss:  1.9365445375442505, processed: 92.90322580645162%\n",
      "Epoch: 0, Loss:  1.5207269191741943, processed: 95.48387096774194%\n",
      "Epoch: 0, Loss:  1.7075897455215454, processed: 98.06451612903226%\n",
      "Epoch: 1, Loss:  1.126011610031128, processed: 0.0%\n",
      "Epoch: 1, Loss:  1.7032136917114258, processed: 2.5806451612903225%\n",
      "Epoch: 1, Loss:  1.7091624736785889, processed: 5.161290322580645%\n",
      "Epoch: 1, Loss:  1.2255767583847046, processed: 7.741935483870968%\n",
      "Epoch: 1, Loss:  1.5647876262664795, processed: 10.32258064516129%\n",
      "Epoch: 1, Loss:  1.4231690168380737, processed: 12.903225806451612%\n",
      "Epoch: 1, Loss:  1.2826757431030273, processed: 15.483870967741936%\n",
      "Epoch: 1, Loss:  1.402417540550232, processed: 18.064516129032256%\n",
      "Epoch: 1, Loss:  1.4624590873718262, processed: 20.64516129032258%\n",
      "Epoch: 1, Loss:  1.2372214794158936, processed: 23.225806451612904%\n",
      "Epoch: 1, Loss:  1.4737176895141602, processed: 25.806451612903224%\n",
      "Epoch: 1, Loss:  1.1461470127105713, processed: 28.387096774193548%\n",
      "Epoch: 1, Loss:  1.3500752449035645, processed: 30.967741935483872%\n",
      "Epoch: 1, Loss:  1.2005188465118408, processed: 33.5483870967742%\n",
      "Epoch: 1, Loss:  1.3072023391723633, processed: 36.12903225806451%\n",
      "Epoch: 1, Loss:  1.4927438497543335, processed: 38.70967741935484%\n",
      "Epoch: 1, Loss:  1.2886998653411865, processed: 41.29032258064516%\n",
      "Epoch: 1, Loss:  1.5729583501815796, processed: 43.87096774193549%\n",
      "Epoch: 1, Loss:  1.2902886867523193, processed: 46.45161290322581%\n",
      "Epoch: 1, Loss:  1.4031920433044434, processed: 49.03225806451613%\n",
      "Epoch: 1, Loss:  1.7387202978134155, processed: 51.61290322580645%\n",
      "Epoch: 1, Loss:  1.265395164489746, processed: 54.19354838709678%\n",
      "Epoch: 1, Loss:  1.1329379081726074, processed: 56.774193548387096%\n",
      "Epoch: 1, Loss:  1.4197698831558228, processed: 59.354838709677416%\n",
      "Epoch: 1, Loss:  0.9405235052108765, processed: 61.935483870967744%\n",
      "Epoch: 1, Loss:  1.564176082611084, processed: 64.51612903225806%\n",
      "Epoch: 1, Loss:  1.6067311763763428, processed: 67.0967741935484%\n",
      "Epoch: 1, Loss:  1.5131317377090454, processed: 69.6774193548387%\n",
      "Epoch: 1, Loss:  1.4360413551330566, processed: 72.25806451612902%\n",
      "Epoch: 1, Loss:  1.2693006992340088, processed: 74.83870967741936%\n",
      "Epoch: 1, Loss:  1.5526679754257202, processed: 77.41935483870968%\n",
      "Epoch: 1, Loss:  1.2561352252960205, processed: 80.0%\n",
      "Epoch: 1, Loss:  1.1384940147399902, processed: 82.58064516129032%\n",
      "Epoch: 1, Loss:  1.6332440376281738, processed: 85.16129032258064%\n",
      "Epoch: 1, Loss:  1.561274528503418, processed: 87.74193548387098%\n",
      "Epoch: 1, Loss:  1.360749363899231, processed: 90.32258064516128%\n",
      "Epoch: 1, Loss:  1.5006189346313477, processed: 92.90322580645162%\n",
      "Epoch: 1, Loss:  1.3003851175308228, processed: 95.48387096774194%\n",
      "Epoch: 1, Loss:  1.3585724830627441, processed: 98.06451612903226%\n",
      "Epoch: 2, Loss:  1.2860064506530762, processed: 0.0%\n",
      "Epoch: 2, Loss:  0.7953344583511353, processed: 2.5806451612903225%\n",
      "Epoch: 2, Loss:  1.0000989437103271, processed: 5.161290322580645%\n",
      "Epoch: 2, Loss:  1.2628122568130493, processed: 7.741935483870968%\n",
      "Epoch: 2, Loss:  1.0036168098449707, processed: 10.32258064516129%\n",
      "Epoch: 2, Loss:  0.8713407516479492, processed: 12.903225806451612%\n",
      "Epoch: 2, Loss:  0.747767448425293, processed: 15.483870967741936%\n",
      "Epoch: 2, Loss:  1.496138095855713, processed: 18.064516129032256%\n",
      "Epoch: 2, Loss:  0.8001951575279236, processed: 20.64516129032258%\n",
      "Epoch: 2, Loss:  1.3097801208496094, processed: 23.225806451612904%\n",
      "Epoch: 2, Loss:  0.9889278411865234, processed: 25.806451612903224%\n",
      "Epoch: 2, Loss:  1.1271904706954956, processed: 28.387096774193548%\n",
      "Epoch: 2, Loss:  1.033756971359253, processed: 30.967741935483872%\n",
      "Epoch: 2, Loss:  1.0831549167633057, processed: 33.5483870967742%\n",
      "Epoch: 2, Loss:  1.276127815246582, processed: 36.12903225806451%\n",
      "Epoch: 2, Loss:  0.7885141372680664, processed: 38.70967741935484%\n",
      "Epoch: 2, Loss:  0.8719329237937927, processed: 41.29032258064516%\n",
      "Epoch: 2, Loss:  1.2635817527770996, processed: 43.87096774193549%\n",
      "Epoch: 2, Loss:  1.1397229433059692, processed: 46.45161290322581%\n",
      "Epoch: 2, Loss:  1.5726040601730347, processed: 49.03225806451613%\n",
      "Epoch: 2, Loss:  1.1241369247436523, processed: 51.61290322580645%\n",
      "Epoch: 2, Loss:  1.1159451007843018, processed: 54.19354838709678%\n",
      "Epoch: 2, Loss:  1.1894233226776123, processed: 56.774193548387096%\n",
      "Epoch: 2, Loss:  0.7368631362915039, processed: 59.354838709677416%\n",
      "Epoch: 2, Loss:  1.5407342910766602, processed: 61.935483870967744%\n",
      "Epoch: 2, Loss:  1.2663567066192627, processed: 64.51612903225806%\n",
      "Epoch: 2, Loss:  0.7923487424850464, processed: 67.0967741935484%\n",
      "Epoch: 2, Loss:  0.7727739810943604, processed: 69.6774193548387%\n",
      "Epoch: 2, Loss:  0.7747653722763062, processed: 72.25806451612902%\n",
      "Epoch: 2, Loss:  1.1110475063323975, processed: 74.83870967741936%\n",
      "Epoch: 2, Loss:  1.2162704467773438, processed: 77.41935483870968%\n",
      "Epoch: 2, Loss:  1.0023930072784424, processed: 80.0%\n",
      "Epoch: 2, Loss:  1.2175772190093994, processed: 82.58064516129032%\n",
      "Epoch: 2, Loss:  1.164226770401001, processed: 85.16129032258064%\n",
      "Epoch: 2, Loss:  1.4456672668457031, processed: 87.74193548387098%\n",
      "Epoch: 2, Loss:  1.0795190334320068, processed: 90.32258064516128%\n",
      "Epoch: 2, Loss:  0.7549695372581482, processed: 92.90322580645162%\n",
      "Epoch: 2, Loss:  1.0108823776245117, processed: 95.48387096774194%\n",
      "Epoch: 2, Loss:  1.0820008516311646, processed: 98.06451612903226%\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T00:05:13.982005Z",
     "start_time": "2024-09-08T00:05:13.822229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'models/job_name.pt')"
   ],
   "id": "8a2f9d331cb4fea5",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T00:05:15.514633Z",
     "start_time": "2024-09-08T00:05:14.001233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # load the model\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = BERTClass()\n",
    "# model.load_state_dict(torch.load('models/job_name.pt'))\n",
    "# model.to(device)"
   ],
   "id": "21f24c26e4cf69eb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (roberta): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(83828, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(2048, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-2): 3 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=312, out_features=248, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation",
   "id": "8ec39c4ebe5f59ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T00:07:30.373742Z",
     "start_time": "2024-09-08T00:05:15.586338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "model.eval()\n",
    "\n",
    "y_preds = []\n",
    "bar = tqdm(valid_loader)\n",
    "with torch.no_grad():\n",
    "    for _, data in enumerate(valid_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets']\n",
    "        bar.update(1)\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        _, preds = torch.max(outputs, dim = 1)\n",
    "        y_preds.extend(preds)"
   ],
   "id": "298f530e01a1a07a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 2902/2903 [02:14<00:00, 21.55it/s]"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T00:07:35.989909Z",
     "start_time": "2024-09-08T00:07:30.591177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_preds_processed = np.array([])\n",
    "\n",
    "for tens in y_preds:\n",
    "    tens = tens.cpu().numpy()\n",
    "    y_preds_processed = np.append(y_preds_processed, tens)"
   ],
   "id": "68373fe413c84689",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T00:07:36.336585Z",
     "start_time": "2024-09-08T00:07:36.102023Z"
    }
   },
   "cell_type": "code",
   "source": "f1_score(y_test_label, y_preds_processed, average='macro')",
   "id": "6f61237d8d2d39ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37442773399930884"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
